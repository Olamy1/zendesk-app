# Zendesk App v2.1 — Insights Setup & Configuration Guide

**Version:** v2.1
**Author:** Olivier Lamy
**Last Updated:** October 2025

---

## ⚙️ Overview

This guide provides detailed setup and configuration instructions for enabling the **Root-Cause Intelligence Layer** and **Meeting Auto-Briefing** features introduced in v2.1 of the Zendesk Reporting App.

---

## 🧩 1. Backend Setup (FastAPI)

### 📁 Directory Structure

```
backend/
 ├─ routers/
 │   ├─ tickets.py
 │   ├─ users.py
 │   └─ insights.py       👈 NEW
 ├─ services/
 │   ├─ zendesk_service.py
 │   ├─ nlp_service.py     👈 NEW (Root Cause logic)
 │   └─ summary_service.py 👈 NEW (Meeting Brief generation)
 └─ main.py
```

### 🧠 1.1 Root-Cause Intelligence Endpoint

**File:** `backend/routers/insights.py`

```python
from fastapi import APIRouter, HTTPException
from backend.services import nlp_service

router = APIRouter()

@router.get("/insights/root_causes")
def get_root_cause_summary():
    try:
        summary = nlp_service.generate_root_cause_summary()
        return summary
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Root cause analysis failed: {e}")
```

**File:** `backend/services/nlp_service.py`

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from collections import Counter

# Simulated ticket data (replace with real Zendesk pull)
def load_tickets():
    return pd.DataFrame([
        {"subject": "Portal login issue", "notes": "User unable to access policy portal."},
        {"subject": "Policy clarification", "notes": "School requesting clarification on process."},
        {"subject": "Portal timeout", "notes": "Timeout error while accessing portal."}
    ])

def generate_root_cause_summary():
    df = load_tickets()
    texts = (df["subject"] + " " + df["notes"]).tolist()

    # TF-IDF Vectorization
    vec = TfidfVectorizer(stop_words="english")
    X = vec.fit_transform(texts)

    # KMeans Clustering
    k = 2  # tune for dataset size
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X)

    clusters = Counter(model.labels_)
    summary = [
        {"topic": f"Cluster {i}", "tickets": count, "change": "+0"}
        for i, count in clusters.items()
    ]

    return {
        "cycle_start": "2025-10-08",
        "cycle_end": "2025-10-21",
        "themes": summary
    }
```

✅ **Test Command:**

```bash
curl http://localhost:8000/api/insights/root_causes
```

---

### 📝 1.2 Meeting Auto-Briefing Endpoint

**File:** `backend/services/summary_service.py`

```python
from datetime import datetime
from backend.utils.helpers import compute_meeting_window


def generate_meeting_brief():
    win = compute_meeting_window()
    return {
        "cycle_start": win["start"],
        "cycle_end": win["end"],
        "summary": (
            f"Between {win['start']} and {win['end']}:\n"
            "• 24 tickets closed (+20%)\n"
            "• 6 reassignments (most to Policy)\n"
            "• 3 reopened (R&A themes)\n"
            "Top Blockers: Portal Access, Data Requests, Clarification Delays."
        )
    }
```

**File:** `backend/routers/insights.py` (continued)

```python
from backend.services import summary_service

@router.get("/insights/meeting_brief")
def get_meeting_brief():
    try:
        return summary_service.generate_meeting_brief()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Meeting briefing failed: {e}")
```

✅ **Test Command:**

```bash
curl http://localhost:8000/api/insights/meeting_brief
```

---

## 🎨 2. Frontend Integration (React ZAF)

### 🧭 2.1 Add Insights Tab

**File:** `frontend/src/App.jsx`

```jsx
<nav className="flex gap-2">
  {navBtn("dashboard", "Dashboard")}
  {navBtn("audit", "Audit Log")}
  {navBtn("insights", "Insights")} {/* 👈 NEW */}
  {navBtn("settings", "Settings")}
</nav>
```

### 📊 2.2 Create Insights Page

**File:** `frontend/src/pages/Insights.jsx`

```jsx
import React, { useEffect, useState } from "react";
import { getRootCauses, getMeetingBrief } from "../services/backendAPI.js";

export default function Insights() {
  const [causes, setCauses] = useState([]);
  const [brief, setBrief] = useState("");

  useEffect(() => {
    (async () => {
      const root = await getRootCauses();
      const mb = await getMeetingBrief();
      setCauses(root.themes || []);
      setBrief(mb.summary);
    })();
  }, []);

  return (
    <div className="space-y-4">
      <h2 className="text-lg font-semibold">Root-Cause Insights</h2>
      <table className="w-full text-sm border">
        <thead className="bg-gray-100">
          <tr><th>Theme</th><th>Tickets</th><th>Δ</th></tr>
        </thead>
        <tbody>
          {causes.map(c => (
            <tr key={c.topic}><td>{c.topic}</td><td>{c.tickets}</td><td>{c.change}</td></tr>
          ))}
        </tbody>
      </table>

      <div className="border-t pt-4">
        <h2 className="text-lg font-semibold">Meeting Briefing</h2>
        <pre className="bg-gray-50 p-3 rounded text-sm whitespace-pre-wrap">{brief}</pre>
      </div>
    </div>
  );
}
```

---

## 🧠 3. API Helpers

**File:** `frontend/src/services/backendAPI.js`

```javascript
export const getRootCauses = () => fetch(`${BASE}/api/insights/root_causes`).then(r => r.json());
export const getMeetingBrief = () => fetch(`${BASE}/api/insights/meeting_brief`).then(r => r.json());
```

---

## 🧪 4. Validation Checklist

| Area                          | Expected Result                          |
| ----------------------------- | ---------------------------------------- |
| `/api/insights/root_causes`   | Returns clusters of issues with counts   |
| `/api/insights/meeting_brief` | Returns summary string with key metrics  |
| Insights Tab                  | Displays both tables and summaries in UI |
| SharePoint Upload             | Summaries exported nightly via Graph API |

---

## 📅 5. Next Phase: v2.2

* Integrate **NLP sentiment analysis** for tone detection in notes.
* Introduce **alert triggers** for trending negative themes.
* Add **multi-cycle trend visualization** in Insights tab.

---

✅ *End of v2.1 Insights Setup Guide*


## 🤖 Optional AI Enhancement — LLM-Powered Insights (v2.2+)

> **Status:** Optional / Future Upgrade
> **Purpose:** Enhance analytical depth and automate executive-style reporting via a locally-hosted Large Language Model (LLM).

---

### 🎯 Goal

Add a local AI summarization and reasoning layer that can:

* Explain recurring patterns and ticket clusters in plain language.
* Write natural-language “meeting briefs” and recommendations.
* Detect emerging trends or anomalies before meetings.

---

### 🧩 Architecture

Tickets → TF-IDF Clustering → LLM Prompt → JSON Summary → Frontend Insights Tab

**Core Idea:**
Keep your current NLP pipeline (TF-IDF, KMeans) for fast, structured grouping, and layer the LLM on top to provide *contextual summaries and recommendations*.

---

### ⚙️ Backend Setup

**File:** `backend/services/ai_summary_service.py`

```python
from datetime import datetime
import requests
from backend.services import nlp_service
from backend.utils.helpers import compute_meeting_window

OLLAMA_URL = "http://localhost:11434/api/generate"  # 🧩 Local Llama3 via Ollama

def generate_ai_brief():
    """Combine root-cause clusters + LLM summary generation."""
    win = compute_meeting_window()
    clusters = nlp_service.generate_root_cause_summary()

    prompt = f"""
    You are an internal DOE operations analyst.
    Using the following clusters, write:
    1. A 2-sentence executive summary.
    2. Top 3 recurring causes with brief context.
    3. One proactive recommendation.

    Meeting window: {win['start']} → {win['end']}
    Clusters: {clusters}
    Respond in concise JSON format with keys:
    summary, causes, recommendation.
    """

    payload = {"model": "llama3", "prompt": prompt, "stream": False}
    resp = requests.post(OLLAMA_URL, json=payload, timeout=120)
    resp.raise_for_status()
    return resp.json()
```

**File:** `backend/routers/insights.py`
(add to existing file)

```python
from backend.services import ai_summary_service

@router.get("/insights/ai_brief")
def get_ai_brief():
    try:
        return ai_summary_service.generate_ai_brief()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI Brief generation failed: {e}")
```

✅ **Local Deployment Tip:**

Install Ollama locally and pull a small model:

```bash
ollama pull llama3
```

Then run the local inference server:

```bash
ollama serve
```

---

### 🖥️ Frontend Integration

**File:** `frontend/src/pages/Insights.jsx`

```jsx
import { getAIBrief } from "../services/backendAPI.js";
import React, { useEffect, useState } from "react";

export default function Insights() {
  const [aiBrief, setAiBrief] = useState(null);

  useEffect(() => {
    (async () => {
      const result = await getAIBrief();
      setAiBrief(result);
    })();
  }, []);

  return (
    <section className="border-t pt-4">
      <h2 className="text-lg font-semibold">AI-Generated Meeting Brief</h2>
      {aiBrief ? (
        <div className="bg-gray-50 p-3 rounded text-sm">
          <p><strong>Summary:</strong> {aiBrief.summary}</p>
          <p><strong>Top Causes:</strong> {aiBrief.causes.join(", ")}</p>
          <p><strong>Recommendation:</strong> {aiBrief.recommendation}</p>
        </div>
      ) : (
        <p className="text-gray-500">Loading AI insights…</p>
      )}
    </section>
  );
}
```

**File:** `frontend/src/services/backendAPI.js`

```javascript
export const getAIBrief = () =>
  fetch(`${BASE}/api/insights/ai_brief`).then((r) => r.json());
```

---

### ✅ Pros

| Benefit                 | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| **Contextual insight**  | Converts ticket clusters into clear human-readable narratives. |
| **Proactive detection** | Surfaces hidden patterns before escalation.                    |
| **Time savings**        | Auto-generates meeting prep in seconds.                        |
| **On-prem security**    | Runs locally — no DOE data leaves your environment.            |

---

### ⚠️ Considerations

| Risk                    | Mitigation                                                |
| ----------------------- | --------------------------------------------------------- |
| **Latency**             | Cache responses or run async background jobs.             |
| **Hallucination**       | Use structured prompts and JSON schema validation.        |
| **Compute load**        | Use `llama3:instruct` or `phi3-mini` for light inference. |
| **Optional dependency** | Keep this behind an `ENABLE_AI_BRIEFING` env flag.        |

---

### 💡 Configuration

In `.env`:

```ini
ENABLE_AI_BRIEFING=true
OLLAMA_URL=http://localhost:11434/api/generate
LLM_MODEL=llama3
```

---

### 📈 Future Expansion (v2.3+)

* Add retrieval grounding (policy PDFs, historical summaries).
* Implement feedback fine-tuning (director thumbs-up/down training).
* Integrate sentiment analysis for trend detection.

---

✅ **End of Optional AI Enhancement Section**
